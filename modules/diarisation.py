# modules/diarization.py
import whisperx
import tempfile
import os
import torch
import functools
from typing import List, Tuple, BinaryIO, Dict, Optional, Literal, NamedTuple
from dataclasses import dataclass
from modules.logger import logger
import numpy as np
from pathlib import Path

# Type definitions
ModelSize = Literal["tiny", "base", "small", "medium", "large", "large-v2", "large-v3"]

@dataclass
class SpeakerStats:
    """Statistics for individual speakers."""
    speaker_id: str
    word_count: int
    talk_time_seconds: float
    talk_time_percentage: float
    segment_count: int

@dataclass
class DiarizationResult:
    """Complete diarization result with speaker segments and statistics."""
    speaker_segments: List[Tuple[str, str]]  # (speaker_label, text)
    speaker_stats: List[SpeakerStats]
    total_duration: float
    total_words: int

class AudioChunk(NamedTuple):
    """Represents a chunk of audio for processing."""
    start_time: float
    end_time: float
    audio_data: np.ndarray

# Global model cache to avoid reloading
_MODEL_CACHE: Dict[str, any] = {}
_ALIGN_MODEL_CACHE: Dict[str, Tuple[any, any]] = {}
_DIARIZATION_MODEL_CACHE: Dict[str, any] = {}

@functools.lru_cache(maxsize=4)
def get_cached_whisper_model(model_size: ModelSize, device: str):
    """Cache WhisperX models globally to avoid reloading."""
    cache_key = f"{model_size}_{device}"
    
    if cache_key not in _MODEL_CACHE:
        try:
            logger.info(f"üîÑ Loading WhisperX model ({model_size}) on {device}...")
            model = whisperx.load_model(model_size, device=device)
            _MODEL_CACHE[cache_key] = model
            logger.info(f"‚úÖ WhisperX model ({model_size}) cached successfully")
        except Exception as e:
            logger.exception(f"‚ùå Failed to load WhisperX model ({model_size})")
            raise RuntimeError(f"Could not load WhisperX model ({model_size})") from e
    
    return _MODEL_CACHE[cache_key]

@functools.lru_cache(maxsize=8)
def get_cached_align_model(language_code: str, device: str):
    """Cache alignment models by language."""
    cache_key = f"{language_code}_{device}"
    
    if cache_key not in _ALIGN_MODEL_CACHE:
        try:
            logger.info(f"üîÑ Loading alignment model for {language_code}...")
            model_a, metadata = whisperx.load_align_model(language_code=language_code, device=device)
            _ALIGN_MODEL_CACHE[cache_key] = (model_a, metadata)
            logger.info(f"‚úÖ Alignment model for {language_code} cached successfully")
        except Exception as e:
            logger.exception(f"‚ùå Failed to load alignment model for {language_code}")
            raise RuntimeError(f"Could not load alignment model for {language_code}") from e
    
    return _ALIGN_MODEL_CACHE[cache_key]

@functools.lru_cache(maxsize=2)
def get_cached_diarization_model(device: str, auth_token: Optional[str] = None):
    """Cache diarization models."""
    cache_key = f"diarization_{device}_{bool(auth_token)}"
    
    if cache_key not in _DIARIZATION_MODEL_CACHE:
        try:
            logger.info(f"üîÑ Loading diarization model on {device}...")
            model = whisperx.DiarizationPipeline(use_auth_token=auth_token, device=device)
            _DIARIZATION_MODEL_CACHE[cache_key] = model
            logger.info("‚úÖ Diarization model cached successfully")
        except Exception as e:
            logger.exception("‚ùå Failed to load diarization model")
            raise RuntimeError("Could not load diarization model") from e
    
    return _DIARIZATION_MODEL_CACHE[cache_key]

class DiarizationService:
    def __init__(
        self, 
        device: Optional[str] = None, 
        model_size: ModelSize = "medium",
        auth_token: Optional[str] = None,
        chunk_length_minutes: float = 25.0
    ):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.model_size = model_size
        self.auth_token = auth_token
        self.chunk_length_seconds = chunk_length_minutes * 60
        
        # Load and cache models during initialization
        try:
            self.model = get_cached_whisper_model(self.model_size, self.device)
            logger.info(f"‚úÖ DiarizationService initialized with {model_size} model on {self.device}")
        except Exception as e:
            logger.exception("‚ùå Failed to initialize DiarizationService")
            raise

    def _chunk_audio(self, audio_path: str, audio_data: np.ndarray) -> List[AudioChunk]:
        """Split long audio into manageable chunks."""
        sample_rate = 16000  # WhisperX standard sample rate
        total_duration = len(audio_data) / sample_rate
        
        if total_duration <= self.chunk_length_seconds:
            return [AudioChunk(0, total_duration, audio_data)]
        
        chunks = []
        chunk_samples = int(self.chunk_length_seconds * sample_rate)
        
        for i in range(0, len(audio_data), chunk_samples):
            start_time = i / sample_rate
            end_time = min((i + chunk_samples) / sample_rate, total_duration)
            chunk_data = audio_data[i:i + chunk_samples]
            chunks.append(AudioChunk(start_time, end_time, chunk_data))
        
        logger.info(f"üì¶ Split {total_duration:.1f}s audio into {len(chunks)} chunks")
        return chunks

    def _relabel_speakers(self, speaker_segments: List[Tuple[str, str]]) -> List[Tuple[str, str]]:
        """Convert SPEAKER_0, SPEAKER_1 to Speaker 1, Speaker 2."""
        speaker_mapping = {}
        next_speaker_num = 1
        
        relabeled = []
        for original_speaker, text in speaker_segments:
            if original_speaker not in speaker_mapping:
                if original_speaker.startswith("SPEAKER_"):
                    speaker_mapping[original_speaker] = f"Speaker {next_speaker_num}"
                    next_speaker_num += 1
                else:
                    speaker_mapping[original_speaker] = original_speaker
            
            new_label = speaker_mapping[original_speaker]
            relabeled.append((new_label, text))
        
        return relabeled

    def _calculate_speaker_stats(
        self, 
        word_segments: List[Dict], 
        total_duration: float
    ) -> List[SpeakerStats]:
        """Calculate comprehensive speaker statistics."""
        speaker_data = {}
        
        for word_info in word_segments:
            speaker = word_info.get("speaker", "Unknown")
            word = word_info.get("word", "")
            start_time = word_info.get("start", 0)
            end_time = word_info.get("end", 0)
            duration = end_time - start_time
            
            if speaker not in speaker_data:
                speaker_data[speaker] = {
                    "words": [],
                    "total_time": 0.0,
                    "segments": 0
                }
            
            speaker_data[speaker]["words"].append(word)
            speaker_data[speaker]["total_time"] += duration
            speaker_data[speaker]["segments"] += 1
        
        # Convert to SpeakerStats objects
        stats = []
        total_words = sum(len(data["words"]) for data in speaker_data.values())
        
        for speaker, data in speaker_data.items():
            # Relabel speaker names
            if speaker.startswith("SPEAKER_"):
                try:
                    speaker_num = int(speaker.split("_")[1]) + 1
                    display_name = f"Speaker {speaker_num}"
                except (IndexError, ValueError):
                    display_name = speaker
            else:
                display_name = speaker
            
            talk_time_percentage = (data["total_time"] / total_duration * 100) if total_duration > 0 else 0
            
            stats.append(SpeakerStats(
                speaker_id=display_name,
                word_count=len(data["words"]),
                talk_time_seconds=data["total_time"],
                talk_time_percentage=talk_time_percentage,
                segment_count=data["segments"]
            ))
        
        # Sort by talk time (descending)
        stats.sort(key=lambda x: x.talk_time_seconds, reverse=True)
        return stats

    def _process_single_chunk(
        self, 
        chunk: AudioChunk, 
        tmp_path: str, 
        chunk_index: int
    ) -> Tuple[List[Dict], str]:
        """Process a single audio chunk."""
        try:
            logger.info(f"üîÑ Processing chunk {chunk_index + 1}: {chunk.start_time:.1f}s - {chunk.end_time:.1f}s")
            
            # Transcribe
            transcription = self.model.transcribe(chunk.audio_data, batch_size=16)
            
            if "segments" not in transcription or not transcription["segments"]:
                logger.warning(f"‚ö†Ô∏è No segments found in chunk {chunk_index + 1}")
                return [], transcription.get("language", "en")
            
            # Align
            language = transcription.get("language", "en")
            model_a, metadata = get_cached_align_model(language, self.device)
            aligned_transcription = whisperx.align(
                transcription["segments"], model_a, metadata, chunk.audio_data, self.device
            )
            
            # Diarize
            diarize_model = get_cached_diarization_model(self.device, self.auth_token)
            diarize_segments = diarize_model(tmp_path, min_speakers=1, max_speakers=10)
            
            # Assign speakers
            result = whisperx.assign_word_speakers(diarize_segments, aligned_transcription["word_segments"])
            
            # Adjust timestamps for chunk offset
            word_segments = result.get("word_segments", [])
            for segment in word_segments:
                if "start" in segment:
                    segment["start"] += chunk.start_time
                if "end" in segment:
                    segment["end"] += chunk.start_time
            
            return word_segments, language
            
        except Exception as e:
            logger.exception(f"‚ùå Failed to process chunk {chunk_index + 1}")
            return [], "en"

    def transcribe_and_diarize(
        self, 
        file: BinaryIO, 
        extension: str = "mp3"
    ) -> DiarizationResult:
        """
        Transcribe audio and segment by speaker with comprehensive analysis.
        Returns DiarizationResult with segments, statistics, and metadata.
        """
        if extension not in ['mp3', 'wav', 'm4a', 'webm', 'flac', 'ogg']:
            logger.error(f"Unsupported audio format: {extension}")
            raise ValueError(f"Unsupported audio format: .{extension}")
        
        tmp_path = None
        try:
            # Save to temporary file
            with tempfile.NamedTemporaryFile(delete=False, suffix=f".{extension}") as tmp:
                file.read()
                file.seek(0)  # Reset file pointer
                tmp.write(file.read())
                tmp_path = tmp.name
            
            logger.info(f"üéµ Processing audio file: {tmp_path}")
            
            # Load audio
            audio_data = whisperx.load_audio(tmp_path)
            total_duration = len(audio_data) / 16000  # WhisperX uses 16kHz
            
            # Chunk audio for long files
            chunks = self._chunk_audio(tmp_path, audio_data)
            
            # Process all chunks
            all_word_segments = []
            detected_language = "en"
            
            for i, chunk in enumerate(chunks):
                # Save chunk to temporary file for diarization
                chunk_tmp_path = None
                try:
                    with tempfile.NamedTemporaryFile(delete=False, suffix=f".{extension}") as chunk_tmp:
                        # Write chunk audio data (this is simplified - in practice you'd need proper audio encoding)
                        chunk_tmp_path = chunk_tmp.name
                        # For WhisperX, we can work directly with the original file and time offsets
                        word_segments, language = self._process_single_chunk(chunk, tmp_path, i)
                        all_word_segments.extend(word_segments)
                        detected_language = language
                finally:
                    if chunk_tmp_path and os.path.exists(chunk_tmp_path):
                        os.remove(chunk_tmp_path)
            
            if not all_word_segments:
                logger.warning("‚ö†Ô∏è No word segments found in any chunks")
                return DiarizationResult(
                    speaker_segments=[("Unknown", "No speech detected")],
                    speaker_stats=[],
                    total_duration=total_duration,
                    total_words=0
                )
            
            # Group words by speaker
            speaker_map = {}
            for word_info in all_word_segments:
                speaker = word_info.get("speaker", "Unknown")
                word_text = word_info.get("word", "").strip()
                if word_text:  # Only add non-empty words
                    speaker_map.setdefault(speaker, []).append(word_text)
            
            # Create speaker segments
            speaker_segments = [(speaker, " ".join(words)) for speaker, words in speaker_map.items()]
            
            # Relabel speakers to human-friendly names
            speaker_segments = self._relabel_speakers(speaker_segments)
            
            # Calculate statistics
            speaker_stats = self._calculate_speaker_stats(all_word_segments, total_duration)
            total_words = sum(stat.word_count for stat in speaker_stats)
            
            logger.info(f"üó£Ô∏è Diarization complete: {len(speaker_segments)} speakers, {total_words} words, {total_duration:.1f}s")
            
            return DiarizationResult(
                speaker_segments=speaker_segments,
                speaker_stats=speaker_stats,
                total_duration=total_duration,
                total_words=total_words
            )
            
        except Exception as e:
            logger.exception("‚ùå Diarization failed")
            return DiarizationResult(
                speaker_segments=[("Unknown", "Diarization failed")],
                speaker_stats=[],
                total_duration=0.0,
                total_words=0
            )
        finally:
            if tmp_path and os.path.exists(tmp_path):
                os.remove(tmp_path)
                logger.debug(f"üóëÔ∏è Cleaned up temporary file: {tmp_path}")

    def get_speaker_summary(self, result: DiarizationResult) -> str:
        """Generate a human-readable summary of speaker contributions."""
        if not result.speaker_stats:
            return "No speakers detected."
        
        summary_lines = [f"üìä Speaker Analysis ({result.total_duration:.1f}s total, {result.total_words} words):\n"]
        
        for i, stats in enumerate(result.speaker_stats, 1):
            summary_lines.append(
                f"{i}. {stats.speaker_id}: "
                f"{stats.word_count} words ({stats.word_count/result.total_words*100:.1f}%), "
                f"{stats.talk_time_seconds:.1f}s ({stats.talk_time_percentage:.1f}%), "
                f"{stats.segment_count} segments"
            )
        
        return "\n".join(summary_lines)

    def clear_cache(self):
        """Clear all cached models to free memory."""
        global _MODEL_CACHE, _ALIGN_MODEL_CACHE, _DIARIZATION_MODEL_CACHE
        _MODEL_CACHE.clear()
        _ALIGN_MODEL_CACHE.clear()
        _DIARIZATION_MODEL_CACHE.clear()
        
        # Clear function caches
        get_cached_whisper_model.cache_clear()
        get_cached_align_model.cache_clear()
        get_cached_diarization_model.cache_clear()
        
        logger.info("üßπ All diarization model caches cleared")